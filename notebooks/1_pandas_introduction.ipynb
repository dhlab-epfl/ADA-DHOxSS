{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter I - Pandas (Introduction)\n",
    "\n",
    "In this notebook we will cover how to:\n",
    "- work with the two main data types in `pandas`: `DataFrame` and `Series`\n",
    "- work with data types in `pandas`, especially strings and dates\n",
    "- load data from JSON and CSV into a `DataFrame`\n",
    "- manipulate the columns of a `DataFrame`\n",
    "- access data in a `DataFrame` by means of indexes and slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/2024-08-13_data.txt', header=None)\n",
    "df.columns=['ID', 'Name', 'Math', 'Science', 'English']\n",
    "df['average'] = df[['Math', 'Science', 'English']].mean(axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section (1): Creating a first dataframe from scratch\n",
    "\n",
    "This section will take you through the steps needed to create a `pandas` DataFrame from scratch.\n",
    "\n",
    "To create a DataFrame from scratch, you need the values for at least two columns.\n",
    "Those values are stored in a data type called a `Series`. They can be thought of as the `pandas` version of lists.\n",
    "\n",
    "A pandas `Series` can be created as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Pandas Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([1,2,3])\n",
    "\n",
    "print(s)\n",
    "print(' > The type of s is:', type(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✏️ [Ex.1] \n",
    "- ✏️ Create a series called `s` containing 100 random numbers ranging between 0 and 1. You may use the `random` function.\n",
    "- ✏️ Print the first value of the `Series` you just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: create a list of 100 random values\n",
    "# step 2: convert that list to a Pandas Series\n",
    "# step 3: get the first element of that list\n",
    "\n",
    "# your solution here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each observation in the series has an **index** as well as a set of **values**: they can be accessed via the omonymous properties.\n",
    "- The data type of the **index** is a `pandas RangeIndex`, akin to a Python `range`.\n",
    "- The data type of the **values** is a `numpy array`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✏️ [Ex.2] \n",
    "- ✏️ Using the series **index**, print the length of the `Series`\n",
    "- ✏️ Print the first three elements of the **values** of series `s`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas `Series` have got useful properties that you can call to easily access information on the data in the Series.\n",
    "Some of them include:\n",
    "- `head(n)` and `tail(n)` to access the beginning and end of the series — where `n` is the number of values to get.\n",
    "- `value_counts()` to show the occurrences of all values in the series. Calling this property returns a `Counter` object, itself contains an `.index` and some `.values` which you can call to access the occurrences' count.\n",
    "- `min()`, `max()`, `mean()`, `median()` give some basic statistics on the series' data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✏️ [Ex.3] \n",
    "- ✏️ Calculate the range of values in `s`\n",
    "- ✏️ Find if there are some duplicate values in `s`\n",
    "- ✏️ Calculate the mean of the first 50 values in `s`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Some of you might want to manipulate time data in the form of dates. Pandas is very convenient for the manipulation of dates. \n",
    "\n",
    "To do that, you should use pandas appropriate date type, called `Timestamp`.\n",
    "\n",
    "For example, VE-day can be encoded as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.Timestamp(1945, 5, 8, 20, 10, 56))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A date can also be encoded as a string, and pandas will do its best to convert it to a timestamp.\n",
    "\n",
    "Note that it flexibly supports both 'YYYYMMDD' and 'YYYMMDDHHMMSS' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.Timestamp('19450508'))\n",
    "print(pd.Timestamp('19690711025615'))\n",
    "\n",
    "# What happens if you try to create a Timestamp with a date that doesn't exist? Try it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between two `Timestamps` is a `Timedelta` object. The number of days contained in the time difference can be accessed through the eponymous property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((pd.Timestamp('19690711025615') - pd.Timestamp('-19450508')).days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A date can be shifted simply by adding to it a `Timedelta`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.Timestamp('19450508')+pd.Timedelta('55 days 2 hours 15 minutes 10 seconds'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✏️ [Ex.4] \n",
    "- ✏️ Create a list of pandas `Timestamps` of all the days between the 24th May 1819 and the 22nd January 1901.\n",
    "- ✏️ By converting this list into a pandas `Series`, get the median day of this time interval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a `pandas.DataFrame`? Think of it as an in-memory spreadsheet that you can analyse and manipulate programmatically.\n",
    "\n",
    "A `DataFrame` is a collection of `Series` having the same length and whose indexes are in sync. A *collection* means that each column of a dataframe is a series\n",
    "\n",
    "Let's create a toy `DataFrame` by hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [pd.Timestamp(1970, 5, 23), pd.Timestamp(1978, 7, 14), pd.Timestamp(1986, 3, 14), pd.Timestamp(1993, 1, 1), pd.Timestamp(1998, 7, 14)]\n",
    "events = ['birth', 'anniversary', 'wedding', 'wedding', 'anniversary']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From those two lists, you can create a `DataFrame` by passing to `pd.DataFrame` a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df = pd.DataFrame({\n",
    "    \"date\": dates,\n",
    "    \"event\": events\n",
    "})\n",
    "\n",
    "# What do you expect when dates and events are changed from lists to Series? Try it out.\n",
    "# What will happen if the lists are of different lengths?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check that the `DataFrame` has been properly constructed. Notice how it is indeed of a tabular shape. To extract its length, you can use `len(DataFrame)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('> This DataFrame has length:', len(toy_df))\n",
    "display(toy_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✏️ [Ex.5] \n",
    "- ✏️ Create a list of pandas `Timestamps` of 200 random dates between the 1900 and 2000.\n",
    "- ✏️ Create a list of 200 events taken *at random* among ['birth', 'anniversary', 'wedding']. The numpy function `np.random.choice()` might help.\n",
    "- ✏️ Combine those two lists to create a dataframe\n",
    "- ✏️ Get the count of occurrences of all **events** in the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the `DataFrame` exists, you can add a column in exactly the same way you would define a new key/value pair in a dictionnary:\n",
    "\n",
    "`dataframe_name['new_column'] = values`\n",
    "\n",
    "\n",
    "Here, the datatype of `values` is quite flexible as `Pandas` allows many inputs: `pandas.Series` as we've seen before, but also `numpy.array` or even simple `lists`.\n",
    "\n",
    "The only condition is that the length of the new column should be of the same length as the `DataFrame`.\n",
    "\n",
    "There is one exception to that rule: if all rows of the new column have the same value, you can just pass that value as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✏️ [Ex.6] \n",
    "- ✏️ Add a new column named `author_firstname` to the dataframe created in [Ex.5]. \n",
    "- ✏️ This new column be input using a list-like variable, containing your first name as many times as there are rows.\n",
    "- ✏️ Add a new column named `author_lastname` to the dataframe, this time containing your last name, and without using a list-like input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section (2): First manipulations of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) General information on the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some first pieces of information on a dataframe are given by the following useful functions: `df.head()`, `df.tail()`, `df.info()`.\n",
    "\n",
    "The method `info()` gives you information about a dataframe:\n",
    "- how much space does it take in memory?\n",
    "- what is the datatype of each column?\n",
    "- how many records are there?\n",
    "- how many `null` values does each column contain (!)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(toy_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if you need to know only the number of columns and rows you can use the `.shape` property. \n",
    "\n",
    "It is a property, not a method — therefore it should be called without brackets.\n",
    "\n",
    "Calling the property returns a tuple with 1) number of rows, 2) number of columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`head()` prints by first five rows of a dataframe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the number of lines displayed is a parameter that can be changed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tail()` does the opposite, i.e. prints the last n rows in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may sometimes want to sort the dataframe based on the values in one column.\n",
    "\n",
    "To do this, you may use the `.sort_values(<column>)` method. \n",
    "\n",
    "The column will then be sorted depending on the datatype:\n",
    "- numerically (float, integers);\n",
    "- chronologically (datetimes);\n",
    "- alphabetically (strings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df.sort_values('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to invert the sorting (z-to-a, 9-to-1, etc.), use the `ascending=False` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df.sort_values('event', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Columns and datatype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of a `pandas.DataFrame` can be accessed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df['date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It returns a `pandas.Series`, the type we've seen in the introductory section of this notebook. To access its **values** the property keyword is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(toy_df['date']))\n",
    "print(toy_df['date'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column in a `pandas.DataFrame` has a data type. Being sure that the right datatype is used is essential.\n",
    "\n",
    "Depending on the nature of the data, its type can be changed using the method `.astype()`. \n",
    "\n",
    "For example, changing from a `pandas.Timestamp` to a `str` is possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(toy_df['date'].astype(str)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But changing from a `pandas.Timestamp` to a `float` is not possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What do you expect when you run the following?\n",
    "#print(toy_df['date'].astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Accessor properties\n",
    "\n",
    "For certain data types (string, datetime), `pandas` provides a number of common methods that can be called on any series containing values of that type. These methods become available as methods of the series itself within a property — called *accessor* — named after the data type:\n",
    "\n",
    "- the `.dt.*` accessor contains methods to operate on `datetime` series\n",
    "- the `.str.*` accessor contains methods to operate on `str` (string) series.\n",
    "\n",
    "Accessors are amongst the most convenient features of data manipulation in `pandas`.\n",
    "\n",
    "They act on a `pandas.Series`, typically the column of a `DataFrame` and return a `pandas.Series` of the same length. The output new series is the result of the element-wise operation on the input series.\n",
    "\n",
    "Let's start with temporal manipulation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `datetime` accessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with datetime series `pandas` provide a bunch of useful methods to operate on a series: they can be called from the `.dt` property of a datetime series.\n",
    "\n",
    "They can be used to:\n",
    "- convert from one timezone to another\n",
    "- get the day/day name/month/year information from each date\n",
    "- and much more (see the [documentation]())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df['new_column'] = the_values\n",
    "\n",
    "toy_df['date'].dt.day_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✏️ [Ex.7] \n",
    "To see this in action: \n",
    "- ✏️ Access your dataframe's `date` column\n",
    "- ✏️ Print for each the corresponding day of the week. To do this, you should use the `datetime` accessor, and use the method `day_name`. This will return a `pandas.Series`.\n",
    "- ✏️ Add the day of the week as a new column.\n",
    "- ✏️ Try to do this again in a one-liner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `str` accessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like the `datetime` accessor, the `str` one is the entry door to many very useful methods that you may need to tidy, process, or analyse your data.\n",
    "\n",
    "Among other things, you can easily:\n",
    "- test if the string starts with another string, \n",
    "- convert between lower and upper case, \n",
    "- determine if the string matches a regular expression,\n",
    "- replace one substring with one another.\n",
    "\n",
    "For example, if you want to check if the first three letters of the `event` column are those of \"wedding\", you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df['day_name'].str.endswith('day')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also chain the accessors. However, remember than the output of an accessor method is a `pandas.Series`. You will therefore need to access `str` again!\n",
    "\n",
    "For example, if you want first to capitalise a column, before checking whether it starts with the first letters of \"wedding\", you can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This time, we match with \"Wed\"\n",
    "\n",
    "toy_df['event'].str.capitalize().str.startswith('wed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df = toy_df.rename(columns={'day_name':'day'})\n",
    "toy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✏️ [Ex.8] \n",
    "- ✏️ Using the `str` accessors, create a new column `is_weekend` that states if the date fell during a weekend.\n",
    "- ✏️ The new column should be a boolean (True/False).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section (3): Input/Output\n",
    "\n",
    "Up until now, we have created and manipulated data from scratch.\n",
    "\n",
    "However most often, they are created by loading existing data into a dataframe by means of `pandas`' input/output methods:\n",
    "\n",
    "- Either by loading a complete dataframe, for example if you want to manipulate a CSV file;\n",
    "- Or by loading the data columns independantly, and combining them into one dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From JSON\n",
    "\n",
    "A very common data format is JSON, explicit, efficient, and widely used over the internet.\n",
    "\n",
    "We will take here the example of some data on books from the British Library. We extracted it from the BL as a JSON file. You may face such a scenario in your research.\n",
    "\n",
    "Loading data from a JSON file is very similar to creating a `DataFrame` from a `dict`, like we've done in Section (1).\n",
    "\n",
    "This is how one would do it in pure Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_file_path = '../data/bl_books/sample/book_data_sample.json'\n",
    "\n",
    "# JSON data gets read into a dictionary\n",
    "\n",
    "with open(json_file_path, 'r') as jsonfile:\n",
    "    json_data = json.load(jsonfile)\n",
    "    \n",
    "books_df = pd.DataFrame(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since reading from files is a very common operation in any data analysis workflow, `pandas` provides methods to read from a variety of formats (JSON, CSV, clipboard, etc.)\n",
    "\n",
    "The block of code above can be replaced by the following one-liner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = pd.read_json(json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From CSV\n",
    "\n",
    "Similarly to `pandas.read_json()`, `pandas.read_csv()` is there to make your life easier when it comes to loading CSV data into a dataframe (and that happens very often!).\n",
    "\n",
    "This is particularly useful if you want to export dataframes into files compatible with Excel or other tabular data software. \n",
    "\n",
    "Let's see how to import one of the CSV files from the \"Venice Apprenticeship\" dataset (`../data/apprenticeship_venice/`). \n",
    "\n",
    "They contain information extracted from ~10,000 work contracts in 17th-century Venice, in particular master-apprentice relationships in the glass-industry. The apprentices were nick-named *garzoni*, hence the name of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = '../data/apprenticeship_venice/professions_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the following. Does it work? Before going to the next cell, can you guess why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garzoni_df = pd.read_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the file first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 2 ../data/apprenticeship_venice/professions_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than a comma-separated value, it looks like semicolon-separated values...\n",
    "We thus need to adjust the `sep` parameter to specify which character is used to separate column values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garzoni_df = pd.read_csv(\n",
    "    csv_file_path,\n",
    "    sep=';'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export\n",
    "\n",
    "Once you have a `DataFrame` to play with in you python environment, exporting it is quite straightforward. \n",
    "\n",
    "Each export format has its own method: `.to_csv()`, `.to_json()`, `.to_html()`, etc.\n",
    "\n",
    "The argument of the function is simply the `<path name>`, and sometime optional arguments, like which value separator you want to use for `.csv` files:\n",
    "\n",
    "`<your_dataframe>.to_csv(<export_path>, sep=<which_separator>)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df.to_csv('example_dataframe.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section (4): Accessing and manipulating data.\n",
    "\n",
    "We now have learned to get our hands on larger dataframes, with thousands of rows and dozens of columns, akin to the sort you may manipulate in a typical DH project.\n",
    "\n",
    "It is now time to learn how to access the data stored in those dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "There are two main ways to find information contained in a dataframe cell:\n",
    "- either you know exactly where it is in the dataframe, for example if your frame is in a specific order;\n",
    "- or, you want to access one or more cells based on conditions.\n",
    "\n",
    "Let's take our books dataframe for the first of those cases. If you want to know the 'title' of the n-th row, you can get it using the `.at` keyword. The structure is:\n",
    "\n",
    "`<dataframe>.at[<row_number>, <column_name>]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✏️ [Ex.9]\n",
    "- ✏️ What is the 'title' of the 26th row?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If we take the example of the `books_df` again, we might want to find all books that have been printed in 1841, or perhaps do we want all books printed privately, or perhaps still do we want all books privately printed in 1841:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The property to use to do this is called `.loc`. This is perhaps the most important `pandas` function.\n",
    "\n",
    "It is applied to a `pandas.DataFrame` and returns a subset of that dataframe that matches the conditions given.\n",
    "\n",
    "What is the structure? Retaking the examples phrased earlier, this is what it looks like? Is that structure clear to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One condition\n",
    "messy_data = books_df.loc[~(books_df['datefield'].str.isnumeric())]\n",
    "clean_data = books_df.loc[(books_df['datefield'].str.isnumeric())]\n",
    "\n",
    "print('Clean data: ', len(clean_data))\n",
    "print('Messy data: ', len(messy_data))\n",
    "print(f\"The data to clean is: {100*len(messy_data)/len(books_df)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another condition\n",
    "books_df.loc[books_df['publisher']=='Privately printed']\n",
    "books_df.loc[books_df['datefield']=='1841']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both conditions\n",
    "books_df.loc[(books_df['datefield']=='1841')&(books_df['publisher']=='Privately printed')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditions can be combined using Boolean logic. We can thus for example search for:\n",
    "- Condition A **and** Condition B\n",
    "- Condition A **or** Condition B\n",
    "- Condition A **and not** Condition C\n",
    "- etc.\n",
    "\n",
    "The boolean operators are the same as in traditional Python: `&`, `|`, `~`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✏️ [Ex.10]\n",
    "\n",
    "Using the indexing structure we've just seen, try to find which book(s) obey the following conditions:\n",
    "\n",
    "- ✏️ 1° Published by 'John Murray' in 1856\n",
    "- ✏️ 2° Printed in London in 1818\n",
    "- ✏️ 3° Published between 1830 and 1848\n",
    "- ✏️ 4° Published by 'Longmans & Co.' or 'Henry Colburn'\n",
    "- ✏️ 5° Published in 1894 but not by 'W. Blackwood & Sons'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaN values\n",
    "\n",
    "Throughout your own research, you will most likely stumble upon instances where your data is incomplete: incomplete metadata from an external source, data manipulation exceptions that don't return a value, missing record. \n",
    "\n",
    "This will take the form of a cell containing a `None` or `numpy.nan` value.\n",
    "\n",
    "It is important to know how to filter them out (or in) in `pandas`, especially as they can mess up some functions you might want to apply to the whole dataframe.\n",
    "\n",
    "For example, let's take our earlier toy dataframe and remove one cell;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df.at[2, 'event'] = np.nan\n",
    "toy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find which rows are NA in a specific **column**, we use the following `.isna()` method.\n",
    "\n",
    "It returns a `series` of booleans telling whether the matching row has a nan value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df['event'].isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our `.loc` operator, we can convert that to a subset of the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df.loc[toy_df['event'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empty cells can be filled with one replacement value. For example here, we might want to prefer \"unknown\" to a NaN value.\n",
    "\n",
    "Be careful, `.fillna()` returns a `pandas.Series`, not a full dataframe. Therefore we use its output to recreate a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = toy_df['event'].fillna('not known', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reindexing\n",
    "\n",
    "To change the value of one or more cells, you should know which rows are concerned by the change.\n",
    "Like earlier, this can come either:\n",
    "- from the precise row number (if you know beforehand which row/column cell you want to change)\n",
    "- from a certain condition being met (let's change all cells that start with 'unk', for example)\n",
    "\n",
    "The reindexing can be thought of as two distinct parts:\n",
    "- 1° knowing which rows to change;\n",
    "- 2° modifying the values of these rows for a specific column.\n",
    "\n",
    "To do this, use the following structure:\n",
    "\n",
    "`<dataframe>.loc[<dataframe>[<column_to_test>]==<value_to_check_for>, <column_to_change>] = <new_value>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing with our toy dataframe, this expression selects all rows whose 'event' is 'birth':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df.loc[toy_df['event']=='wedding']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you may want to change that into 'marriage':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df.loc[toy_df['event']=='wedding', 'event'] = 'marriage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But again, the testing column doesn't have to be the one you change.\n",
    "\n",
    "More generally, this is:\n",
    "`<dataframe>.loc[<condition>, <column_to_change>] = <new_value>`\n",
    "\n",
    "For example, say I want to change my last name in our earlier dataframe:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✏️ [Ex. 11]\n",
    "\n",
    "In this exercise, we will combine what we've seen on Indexing, Detecting NA values, and Reindexing.\n",
    "\n",
    "The dataframe of books that was opened from a JSON earlier `books_df` contains information on which is the identifier to a first page scan. This information is in the column `imgs`.\n",
    "In some cases however, the image does not exist and the cell is empty.\n",
    "\n",
    "- ✏️ Find how many books do not have an image (column `imgs`)\n",
    "- ✏️ Find *which* books do not have an image\n",
    "- ✏️ Replace the empty cells that do not have an image with a dummy value\n",
    "- ✏️ (Advanced) Find which are the top four geographical origin for books without an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating\n",
    "\n",
    "As a rule of thumb, one should prioritise any data manipulation that can be performed in one shot. `pandas.DataFrame` are ineffecient objects for frequent modification. \n",
    "\n",
    "However, there are scenarios where you may want to iterate over the rows of the dataframe. For example, to perform row-wise tests or manipulation that require access to more than one column.\n",
    "\n",
    "To do that, the `.iterrows()` method (for \"iterration over rows\") should be used. \n",
    "\n",
    "Just like an `enumerate` in traditional python, the `.iterrows()` method will return two elements at each iteration:\n",
    "- the index of the row (between 0 and the length of the dataframe);\n",
    "- the row corresponding to that iteration.\n",
    "\n",
    "That row is a `pandas.Series`. It acts a bit like a python `dictionary` and the value for a specific column is accessed through:\n",
    "`row[<column>]`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in toy_df.iterrows():\n",
    "    index = i[0]\n",
    "    row = i[1]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in toy_df.iterrows():\n",
    "    print('The event at index', index, 'is:', row['event'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✏️ [Ex. 12]\n",
    "\n",
    "Here we will combine iteration, and `datetime` and `str` accessors. By iterating over the `toy_df` dataframe:\n",
    "\n",
    "- ✏️ Print the date of each row as well\n",
    "- ✏️ Print the day of the week that each date corresponds to\n",
    "- ✏️ Print whether the event starts with 'bir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final exercise: Shakespeare and Company project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**\n",
    "\n",
    "For this excercise, we will be working with an open-access data from a DH research project: the [*Shakespeare and Company project*](https://shakespeareandco.princeton.edu/).\n",
    "\n",
    "The dataset we'll be using contains the list of books that were lent out in the 20th century by the celebrated *Shakespeare and Company* library in Paris.\n",
    "\n",
    "The dataset can be downloaded from the following address:https://dataspace.princeton.edu/handle/88435/dsp01jm214s28p (file size = ~2MB).\n",
    "\n",
    "You may use CSV or JSON:\n",
    "- the CSV file will only contain `str`, `float` and `int`;\n",
    "- whereas the JSON file will contain `lists`.\n",
    "\n",
    "The choice will be one of comfort. Pick whichever yields the datatypes you are more comfortable with.\n",
    " \n",
    "**Steps**\n",
    "\n",
    "Start by loading the file it into a `pandas.DataFrame` called \"SCo\"\n",
    "\n",
    "**Try to answer the following questions**\n",
    "- 1° How many records does it contain?\n",
    "\n",
    "\n",
    "- 2° What's the format(s) of the **most purchased** document(s)? How many times was it/were they purchased?\n",
    "- 3° What's the format(s) of the **least borrowed** document(s)? How many times was it/were they borrowed?\n",
    "\n",
    "- 4° Which is the most borrowed **book**? How many times was it borrowed?\n",
    "\n",
    "- 5° How many 'Poems' are there in collection? What is the earliest one? How many don't have a date?\n",
    "\n",
    "- 6° Replace the empty cells in the `circulation_year` column with \"unknown\".\n",
    "- 7° Find which books were in circulation in 1951\n",
    "- 8° (Advanced) By iterating over the rows of the dataframe, create a dictionnary of dates that gives how many books from the datafram were in circulation that year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dhoxss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
