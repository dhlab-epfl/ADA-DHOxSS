{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Chapter V - Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is aimed to work alongside our explanation to help illustrate how to begin the basics of natural language processing.\n",
    "\n",
    "- Topic modelling\n",
    "- Resources for further research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section A) Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first import our required packages. For the basics of natural language processing we're going to be using nltk. You'll need to install this in your current enviroment using the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure your enviroment is set up, open your terminal. The line should begin with (base).\n",
    "\n",
    "- We have previously used 'conda create -n enviroment_name' to create an enviroment. Now use 'conda activate enviroment_name' to open your enviroment, once you've run this the line should start with (enviroment_name).\n",
    "\n",
    "- Once in the enviroment we can install packages. We want to install them here rather than in the base so that they are only used for this project. \n",
    "\n",
    "- To do this use 'conda install package_name'.\n",
    "In this case 'conda install nltk'.\n",
    "\n",
    "- Note that for smaller packages (eg. pyldavis), we have to specify where the package can be found in this case, the conda-forge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run those three commands in order to install the needed packages\n",
    "\n",
    "# conda activate your_enviroment_name\n",
    "# conda install nltk\n",
    "# conda install gensim\n",
    "# conda install conda-forge::pyldavis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These import statements are different to what we have used so far. Instead of importing a whole library, using '.' allows us to import particular functions and subselections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up imports\n",
    "import pandas as pd, matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "import pyLDAvis.gensim_models\n",
    "import string # This provides us a list of punctuation later down the line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now load in the dataframe and start cleaning. \n",
    "Note: this file is tab seperated (see how this changes read_csv below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/sample_museum_tweets.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Data cleaning and Tokenization\n",
    "\n",
    "Let's check what out data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✏️ [Ex. 1]\n",
    "\n",
    "Let's clean up the datatypes first.\n",
    "- Rename 'original_text' to 'text'\n",
    "- Make sure that the column's datatype is a string\n",
    "- How many null values are there in 'text'?\n",
    "\n",
    "The dataset we have has already been cleaned so there is not much to do! But you may want to be wary of duplicates in your own dataset. The pandas function drop_duplicates() will help. You can check the length of the df before and after to see how many it found. Check the documentation for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'original_text':'text'})\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "# # Note it's always useful to check that there aren't a number of \"NaN\" strings in the column. As there would be more that one, we can identify it by using 'NaN'.\n",
    "# df['text'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our dataset more homogenous, for now we will focus on all of the tweets that are not replies. This is becuase replies tend to have different characteristics (shorter, more boring vocabulary) so we'll drop them for this exercise. \n",
    "\n",
    "Here we can use loc, or simply a tilda (~) for 'not'.\n",
    "\n",
    "When we do this we drop more than half the tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "df = df[~df[\"is_reply\"]]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section B) Preprocessing Tweets\n",
    "https://towardsdatascience.com/lda-topic-modeling-with-tweets-deff37c0e131\n",
    "\n",
    "https://medium.com/swlh/quick-text-pre-processing-c444f0ed9dcc\n",
    "\n",
    "Here we will: \n",
    "- Remove URLs (you could also remove mentions using what we learnt yesterday or the code below, but we will leave them for now)\n",
    "- Remove stop words, punctuation, and lowercase all words\n",
    "- Tokenize each tweet\n",
    "- Remove any remaining special characters\n",
    "- Lemmatize/Stem the tweets\n",
    "\n",
    "To remove urls, we will need to use regex with string operators. Here are some helpful guides:\n",
    "https://blog.hpc.qmul.ac.uk/Beginners-Guide-to-Regular-Expressions/\n",
    "\n",
    "And for a more full description: https://regexone.com/\n",
    "\n",
    "You can test your regex with this site: \n",
    "https://regex101.com/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(df, text_column_name):\n",
    "    df[text_column_name] = df[text_column_name].str.replace(r\"http\\S+\", \"\", regex=True)\n",
    "    return (df)\n",
    "\n",
    "df = remove_urls(df, 'text')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a predefined list of stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Add punctuation to our stopwords list\n",
    "stop_words += list(string.punctuation) # This comes from python itself and is a full list of non-alphabetical characters. e.g. -./:;<=>?@\n",
    "\n",
    "# Add integers\n",
    "stop_words += ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_lowercase_new(text):\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    short_removed = []\n",
    "    for token in tokens:\n",
    "        token_lower = token.lower()\n",
    "        if (token_lower not in stop_words)&(len(token_lower)>3):\n",
    "            short_removed.append(token_lower)\n",
    "    return short_removed\n",
    "\n",
    "df['text_tokens'] = df['text'].apply(tokenize_and_lowercase_new)\n",
    "df['text_tokens'].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Each row is now a list of words that were in that tweet. However, it's a little bit cleaner than the method we used before, as we now don't have any punctuation or common words such as 'a' or 'the'. This is a much better start for analysis. \n",
    "\n",
    "There is one last step, we still need to lemmatize or perform stemming. Lemmatizing is a little bit more sophisticated (remember our example of leaf and leaves) so we will lemmatize using nltk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(df_text):\n",
    "    lemmatized_words =[]\n",
    "    for w in df_text:\n",
    "        lemmatized_words.append(lemmatizer.lemmatize(w))\n",
    "    return lemmatized_words\n",
    "\n",
    "df['text_lemmatized'] = df['text_tokens'].apply(lemmatize_text)\n",
    "df['text_lemmatized'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to save the dataframe for ease of access later.\n",
    "df.to_pickle(\"../../data/lemmatized_museum_tweets.pkl\")  \n",
    "\n",
    "# to read it back to a dataframe we would use: \n",
    "# df = pd.read_pickle(\"../../data/lemmatized_museum_tweets.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=UkmIljRIG_M&t=179s\n",
    "For evalutation of topic models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section C) Basic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../../data/lemmatized_museum_tweets.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw how to visualise the length of museum tweets yesterday, but using nltk, we can do additional checks on the most common words and how they change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "#iterate through each tweet, then each token in each tweet, and store in one list\n",
    "#flat_words = [item for sublist in df['text'] for item in sublist]\n",
    "\n",
    "flat_words = []\n",
    "for sublist in df['text_lemmatized']:\n",
    "    for item in sublist:\n",
    "        flat_words.append(item)\n",
    "\n",
    "word_freq = FreqDist(flat_words)\n",
    "word_freq.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is currently a list of tuples, but we can easily convert it into a dictionary, which we could turn into a wordcloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_30_words = dict(word_freq.most_common(30))\n",
    "top_30_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raf_museums_tweets = df.loc[df['museum_account']=='rafmuseum']\n",
    "horniman_tweets = df.loc[df['museum_account']=='hornimanmuseum']\n",
    "\n",
    "#flat_words_raf = [item for sublist in raf_museums_tweets['text'] for item in sublist]\n",
    "flat_words_raf = []\n",
    "for sublist in raf_museums_tweets['text_lemmatized']:\n",
    "    for item in sublist:\n",
    "        flat_words_raf.append(item)\n",
    "\n",
    "word_freq_raf = FreqDist(flat_words_raf)\n",
    "top_10_words_raf = dict(word_freq_raf.most_common(10))\n",
    "top_10_words_raf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(range(len(top_10_words_raf)), list(top_10_words_raf.values()), align='center')\n",
    "plt.xticks(range(len(top_10_words_raf)), list(top_10_words_raf.keys()))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_words_horn = [item for sublist in horniman_tweets['text_lemmatized'] for item in sublist]\n",
    "word_freq_horn = FreqDist(flat_words_horn)\n",
    "top_10_words_horn = dict(word_freq_horn.most_common(10))\n",
    "\n",
    "plt.bar(range(len(top_10_words_horn)), list(top_10_words_horn.values()), align='center')\n",
    "plt.xticks(range(len(top_10_words_horn)), list(top_10_words_horn.keys()))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section D) Topic modeling\n",
    "\n",
    "So what is our data about? One interesting thing we can try is topic modeling. \n",
    "\n",
    "For this notebook, we're going to be using LDA (Latent Dirichlet Allocation). This is a white box/transparent box model. With this approach you can specifiy the number of clusters, which can be limiting. However, it means that you can label your data manually to create a test dataset to check your parameters agains, just as we discussed earlier this week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we have to create a bag of words\n",
    "\n",
    "#create dictionary\n",
    "text_dict = gensim.corpora.Dictionary(df['text_lemmatized'])\n",
    "\n",
    "#view integer mappings\n",
    "text_dict.token2id\n",
    "\n",
    "tweets_bow = []\n",
    "for tweet in df['text_lemmatized']:\n",
    "    tweets_bow.append(text_dict.doc2bow(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for tokens in df['text_lemmatized']:\n",
    "    for word in tokens:\n",
    "        all_words.append(word)\n",
    "\n",
    "\n",
    "tweet_length_in_lemmas = []\n",
    "for tokens in df['text_lemmatized']:\n",
    "    tweet_length_in_lemmas.append(len(tokens))\n",
    "\n",
    "tweet_length_in_lemmas = [len(tokens) ]\n",
    "vocab = sorted(list(set(all_words)))\n",
    "\n",
    "print('{} words total, with a vocabulary size of {}'.format(len(all_words), len(vocab)))\n",
    "print('Max tweet length is {}'.format(max(tweet_length_in_lemmas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# When we train a model there are a number of parameters that we can add.\n",
    "\n",
    "# k here represents the number of topics\n",
    "k = 7\n",
    "tweets_lda = LdaModel(tweets_bow,\n",
    "                      num_topics = k,\n",
    "                      id2word = text_dict,\n",
    "                      random_state = 1,\n",
    "                      passes=10)\n",
    "\n",
    "tweets_lda.show_topics()\n",
    "tweets_lda.save(\"lda_museum_tweets_7.model\") # The seven here refers to the number of clusters we chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "lda_viz = pyLDAvis.gensim_models.prepare(tweets_lda, tweets_bow, dictionary=tweets_lda.id2word)\n",
    "pyLDAvis.show(lda_viz, local=False)\n",
    "lda_viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the topics look like? Do they make sense to you? We'll try to see if there are \n",
    "\n",
    "✏️ [Ex. 2]\n",
    "\n",
    "Let's clean up the datatypes first.\n",
    "- Write a function that takes the text_dict, the number of clusters (k), and the number of passes and trains and saves a model (don't forget to use f strings to change the name of the file you save each time).\n",
    "- Using your function, try changing the number of clusters to 4, 7, 8 and 10. Which one do you think worked best?\n",
    "- What about the number of passes, how does this change your model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further resources (natural language processing and beyond...)\n",
    "\n",
    "This has been a brief introduction to some basic nlp tasks, but it only scratches the surface. The below links point you in the direction of some helpful resources. They represent a wide range of approaches including neural networks, so do keep in mind what we talked about regarding black box models!\n",
    "\n",
    "### 1) spaCy\n",
    "For further exploration of syntax and named entity recognition, you can explore the spaCy library. They have visualisers to help you get to grips with what it does.\n",
    "https://demos.explosion.ai/displacy-ent\n",
    "https://demos.explosion.ai/displacy\n",
    "\n",
    "I would encourage you to explore the different examples used in the \"Guides\" section of the spaCy documentation.\n",
    "https://spacy.io/usage/linguistic-features\n",
    "https://spacy.io/usage/rule-based-matching\n",
    "\n",
    "It might be helpful for some of your projects!\n",
    "\n",
    "### 2) berTopic\n",
    "Often the go-to tool these days for neural topic modeling. There is a very helpful quickstart guide!\n",
    "https://maartengr.github.io/BERTopic/index.html\n",
    "\n",
    "### 3) Transkribus\n",
    "For those of you who have not processed your scans of text yet and need to do OCR (Optical Character Recognition), Transkribus is to go-to tool for transcribing texts (free for a while, then talk to your university/send them an email). It's a great team and run by academics for academics. Takes some work to figure out all of the functions and interface but it well worth investing the time!\n",
    "\n",
    "Different models be used to work with either handwritten texts or printed/typed, as well as for a wide selection of languages and time periods. \n",
    "https://www.transkribus.org/\n",
    "\n",
    "### 4) Hugging face\n",
    "Most of the libraries mentioned here have a number of pretrained models avaialble (at different sizes), for everything else, there's hugging face.\n",
    "https://huggingface.co/\n",
    "\n",
    "It's not ideal in many ways, but it is the largest repository of pretrained models and datasets currently out there, have an explore! It will be especially helpful for those of you not working in English.\n",
    "An alternative would be kaggle (more for datasets). \n",
    "\n",
    "### 5) Distant viewing lab\n",
    "Fantastic resource for computer vision (step by step guide)\n",
    "https://distantviewing.org/\n",
    "\n",
    "But they also have a section on sentiment analysis using neural networks.\n",
    "https://distantviewing.org/dvscripts/sentiment.html\n",
    "\n",
    "### 6) GLAM workbench \n",
    "A series of amazing notebooks exploring GLAM data. \n",
    "https://glam-workbench.net/\n",
    "\n",
    "I've added some helpful links based on some of your projects! However, feel free to have an explore of the whole site, there are many more notebooks you may find useful. \n",
    "\n",
    "Here they have a guide on how to run these notebooks:\n",
    "\n",
    "https://glam-workbench.net/trove-newspapers/#data-and-images\n",
    "\n",
    "#### Useful notebooks that have direct relevance to some of your projects include: \n",
    "Finding nouns, verbs, sentences, and cleaning your text:\n",
    "https://glam-workbench.net/trove-books/recipe-generator/\n",
    "\n",
    "Geomapping:\n",
    "https://glam-workbench.net/trove-newspapers/Map-newspaper-results-by-place-of-publication/ OR \n",
    "https://glam-workbench.net/trove-newspapers/Map-newspaper-results-by-place-of-publication-over-time/\n",
    "\n",
    "Topic modeling of parlimentary press releases: \n",
    "https://glam-workbench.net/trove-journals/topic-modelling-parliament-press-releases/?h=topic\n",
    "\n",
    "Topic modeling of books:\n",
    "https://glam-workbench.net/trove-books/exploring-digitised-books-adel-rahmani/?h=topic\n",
    "\n",
    "(Alas, should be noted that the trove notebooks aren't being updated due to issues with the National Library of Australia being weird about their API's terms and conditions, but you can still take and adapt them for yourself!)\n",
    "\n",
    "### 7) Plotly\n",
    "Not nlp related but is a different and really nice plotly library where the visualisations are interactive. Worth an experiment: \n",
    "https://plotly.com/python/\n",
    "\n",
    "## Lastly if you come across any great resources yourself feel free to share! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dhoxss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
